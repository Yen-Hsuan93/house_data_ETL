import os
import pandas as pd
import re # <-- æ–°å¢ï¼šéœ€è¦å°å…¥ re æ¨¡çµ„

class FilterBasic:
    """åŸºæœ¬ç¯©é¸èˆ‡æ¬„ä½çµ±ä¸€è™•ç†"""

    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()

    # ç§»é™¤ç‰¹æ®Šå­—å…ƒ/äº‚ç¢¼çš„å‡½å¼

    def remove_pua_chars_from_address(self, address_column: str = "åœŸåœ°ä½ç½®å»ºç‰©é–€ç‰Œ"):
        before_len = len(self.df)
        
        if address_column in self.df.columns:
            PUA_CHAR_PATTERN = r'[\uE000-\uF8FF]'
            rows_to_drop_mask = self.df[address_column].astype(str).str.contains(
                PUA_CHAR_PATTERN, 
                regex=True,
                na=False
            )

            # åˆªé™¤è³‡æ–™åˆ—
            self.df = self.df[~rows_to_drop_mask].copy()

            num_dropped = before_len - len(self.df)
            if num_dropped > 0:
                print(f"[FilterBasic]  åœ°å€æ¸…ç†: åˆªé™¤ {num_dropped} ç­†åŒ…å« PUA äº‚ç¢¼çš„è³‡æ–™ã€‚")
            else:
                print("[FilterBasic]  åœ°å€æ¸…ç†: æœªç™¼ç¾éœ€è¦åˆªé™¤çš„ PUA äº‚ç¢¼ã€‚")
        else:
            print(f"[FilterBasic]  åœ°å€æ¬„ä½ '{address_column}' ä¸å­˜åœ¨ï¼Œè·³éæ¸…ç†ã€‚")
            
        return self

    def drop_duplicates_by_id(self):
        """åˆªé™¤é‡è¤‡ç·¨è™Ÿ"""
        if "ç·¨è™Ÿ" in self.df.columns:
            before = len(self.df)
            self.df = self.df.drop_duplicates(subset=["ç·¨è™Ÿ"], keep="first")
            print(f"[FilterBasic] ç§»é™¤é‡è¤‡ç­†æ•¸: {before - len(self.df)}")
        return self
    
    def unify_columns(self):
        print("[FilterBasic] æ­£åœ¨çµ±ä¸€æ¬„ä½æ ¼å¼...")

        col_new = "è»Šä½ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º"
        col_old = "è»Šä½ç§»è½‰ç¸½é¢ç©(å¹³æ–¹å…¬å°º)"

        # 1. å…ˆç¢ºä¿å…©å€‹æ¬„ä½è‹¥å­˜åœ¨ï¼Œéƒ½è½‰ç‚ºæ•¸å­— (æ‰èƒ½é‹ç®—)
        for col in [col_new, col_old, "å»ºç‰©ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º"]:
            if col in self.df.columns:
                self.df[col] = pd.to_numeric(self.df[col], errors="coerce")

        # 2. è™•ç†åˆä½µé‚è¼¯
        if col_old in self.df.columns:
            if col_new in self.df.columns:
                # ç‹€æ³ A: å…©å€‹æ¬„ä½éƒ½æœ‰ -> åˆä½µè³‡æ–™ (ç”¨èˆŠè£œæ–°)
                self.df[col_new] = self.df[col_new].combine_first(self.df[col_old])
                self.df.drop(columns=[col_old], inplace=True)
                print(f"   -> åˆä½µ [{col_old}] è‡³ [{col_new}]")
            else:
                # ç‹€æ³ B: åªæœ‰èˆŠæ¬„ä½ -> ç›´æ¥æ”¹å
                self.df.rename(columns={col_old: col_new}, inplace=True)
                print(f"   -> æ”¹å [{col_old}] ç‚º [{col_new}]")

        return self


    # éæ¿¾æ¬„ä½
    def filter_out_transaction_targets(self, banned=("åœŸåœ°", "è»Šä½")):
        self.df = self.df[~self.df["äº¤æ˜“æ¨™çš„"].isin(list(banned))]
        return self

    def keep_residential_usage(self, allowed=("ä½å®¶ç”¨", "åœ‹æ°‘ä½å®…", "é›†åˆä½å®…")):
        self.df = self.df[self.df["ä¸»è¦ç”¨é€”"].isin(list(allowed))]
        return self

    def keep_urban_zone_residential(self, allowed=("ä½",)):
        self.df = self.df[self.df["éƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€"].isin(list(allowed))]
        return self

    def filter_out_non_urban_zones(self, exclude=("ä¸€èˆ¬è¾²æ¥­å€", "é„‰æ‘å€", "å±±å¡åœ°ä¿è‚²å€", "ç‰¹å®šè¾²æ¥­å€")):
        col = "ééƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€"
        if col in self.df.columns:
            self.df = self.df[~self.df[col].isin(list(exclude))]
        return self

    def remove_notes_with_keywords(self, keywords=("è¦ªå‹", "è¦ªæˆš", "æœ‹å‹", "å“¡å·¥", "ç‰¹æ®Š")):
        if "å‚™è¨»" in self.df.columns:
            pattern = "|".join(map(str, keywords))
            self.df = self.df[~self.df["å‚™è¨»"].astype(str).str.contains(pattern, na=False)]
        return self
        
    def cleaning_house_type(self,target_types =('ä½å®…å¤§æ¨“', 'è¯å»ˆ', 'å…¬å¯“', 'é€å¤©å','å¥—æˆ¿') ):
        self.df['å»ºç‰©å‹æ…‹'] = self.df['å»ºç‰©å‹æ…‹'].astype(str).str.split('(').str[0]
        self.df = self.df[self.df['å»ºç‰©å‹æ…‹'].isin(target_types)].copy()
        return self

    def add_city_from_source(self):
        mapping = {"F": "æ–°åŒ—å¸‚", "H": "æ¡ƒåœ’å¸‚", "A": "è‡ºåŒ—å¸‚"}
        if "ä¾†æºæª”å" in self.df.columns:
            self.df["æ¨™çš„ç¸£å¸‚"] = self.df["ä¾†æºæª”å"].astype(str).str[0].map(mapping)
        return self
        
def main():

    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
    input_path = os.path.join(PROJECT_ROOT, "main_house_rawdata", "merged_rawdata.csv")
    
    # å»ºè­°å…ˆè™•ç†ç·¨ç¢¼å•é¡Œï¼Œå†è®€å– CSV
    # é€™æ˜¯ç‚ºäº†é¿å…åœ¨è®€å–æª”æ¡ˆæ™‚ï¼Œç‰¹æ®Šå­—å…ƒå°±è®Šæˆç„¡æ³•ä¿®å¾©çš„äº‚ç¢¼
    try:
        # å‡è¨­æ‚¨çš„ CSV æ˜¯ UTF-8 ç·¨ç¢¼ (æ¨è–¦)
        df = pd.read_csv(input_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        # å¦‚æœä¸æ˜¯ UTF-8ï¼Œå‰‡å˜—è©¦ CP950
        df = pd.read_csv(input_path, encoding="cp950")


    filter_basic = FilterBasic(df)
    filter_basic = filter_basic.drop_duplicates_by_id()
    filter_basic = filter_basic.unify_columns()
    
    # ğŸŒŸ åœ¨å…¶ä»–ç¯©é¸ä¹‹å‰å…ˆç§»é™¤äº‚ç¢¼åœ°å€
    filter_basic = filter_basic.remove_pua_chars_from_address()
    
    filter_basic = filter_basic.filter_out_transaction_targets()
    filter_basic = filter_basic.keep_residential_usage()
    filter_basic = filter_basic.keep_urban_zone_residential()
    filter_basic = filter_basic.filter_out_non_urban_zones()
    filter_basic = filter_basic.remove_notes_with_keywords()
    filter_basic = filter_basic.add_city_from_source()

    df = filter_basic.df
    
    cols = ["äº¤æ˜“æ¨™çš„", "ä¸»è¦ç”¨é€”", "éƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€", "ä¾†æºæª”å"] 
    
    for col in cols:
        print(f"\nã€{col}ã€‘ç¨®é¡ï¼š")
        # ç¢ºä¿åœ¨å°å‡ºæ™‚èƒ½è™•ç† NaN å€¼ï¼Œé¿å…éŒ¯èª¤
        print(filter_basic.df[col].dropna().unique())

if __name__ == "__main__":
    main()